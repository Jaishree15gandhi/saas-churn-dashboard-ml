# -*- coding: utf-8 -*-
"""powerbimle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLjEEKSGDiZLL81nZZJIMmxf2Z2PFq90
"""

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# 2. Load Dataset
df = pd.read_csv('Cleaned_SaaS_Churn_Dataset.csv')

# 3. Feature Engineering
df['tenure_bin'] = pd.cut(df['tenure_months'], bins=[0, 6, 12, 24, np.inf], labels=['<6m', '6-12m', '1-2y', '>2y'])

# 4. Define Features and Target
X = df.drop(columns=['churn', 'churn_flag', 'user_id', 'signup_date', 'last_active_date', 'features_used'])
y = df['churn_flag']

# 5. Identify Feature Types
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

# 6. Preprocessing
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
])

# 7. Encode & Balance
X_encoded = preprocessor.fit_transform(X)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_encoded, y)

# 8. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, stratify=y_resampled, test_size=0.2, random_state=42)

# 9. Define XGBoost Model with Tuned Parameters
model = XGBClassifier(
    n_estimators=300,
    learning_rate=0.03,
    max_depth=5,
    min_child_weight=1,
    subsample=0.9,
    colsample_bytree=0.9,
    gamma=0.2,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# 10. Train Model
model.fit(X_train, y_train)

# 11. Predict
y_pred = model.predict(X_test)

# 12. Evaluation
print("âœ… Final Tuned XGBoost Model Evaluation")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
from matplotlib.colors import LinearSegmentedColormap


# 13. Plot Confusion Matrix
from sklearn.metrics import ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=['Not Churned', 'Churned'])
disp.plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

# 14. Feature Importance
ohe = preprocessor.named_transformers_['cat']
ohe_features = ohe.get_feature_names_out(categorical_features)
all_features = numeric_features + list(ohe_features)

importances = model.feature_importances_
importance_df = pd.DataFrame({
    'Feature': all_features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)
print(importances)
print(all_features)

# Plot Top 10 Features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='magma')
plt.title('Top 10 Feature Importances')
plt.tight_layout()
plt.show()

# 15. Optional: Stratified K-Fold Cross Validation Score (F1)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_resampled, y_resampled, cv=skf, scoring='f1')
print(f"ðŸ“Š Mean F1 from Stratified 5-Fold CV: {cv_scores.mean():.4f}")

